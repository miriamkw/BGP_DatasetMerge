{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Quality"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69116b9f07a78cf1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "168fb2addf881c92"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T15:31:01.213820Z",
     "start_time": "2024-11-03T15:31:00.612717Z"
    }
   },
   "id": "a8f9d400752a725c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc2c449fd2ba1424"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    folder_path = os.path.join('../', 'processed_data')\n",
    "    datasets = {}\n",
    "    for dataset_folder in os.listdir(folder_path):\n",
    "        print(dataset_folder)\n",
    "        dataset_folder_path_train = os.path.join(folder_path, dataset_folder, \"train\")\n",
    "        dataset_folder_path_test = os.path.join(folder_path, dataset_folder, \"test\")\n",
    "    \n",
    "        df_train_list = []\n",
    "        df_test_list = []\n",
    "        for subject_data in os.listdir(dataset_folder_path_train):\n",
    "            if subject_data.endswith('.csv'):\n",
    "                file_path = os.path.join(dataset_folder_path_train, subject_data)\n",
    "                df = pd.read_csv(file_path, parse_dates=['date'], index_col='date', low_memory=False)\n",
    "    \n",
    "                df['id'] = subject_data.split(\".\")[0]\n",
    "                #df.set_index('date', inplace=True)\n",
    "                df_train_list.append(df)\n",
    "    \n",
    "        for subject_data in os.listdir(dataset_folder_path_test):\n",
    "            if subject_data.endswith('.csv'):       \n",
    "                file_path = os.path.join(dataset_folder_path_test, subject_data)\n",
    "                df = pd.read_csv(file_path, parse_dates=['date'], index_col='date', low_memory=False)\n",
    "    \n",
    "                df['id'] = subject_data.split(\".\")[0]\n",
    "                #df.set_index('date', inplace=True)\n",
    "                df_test_list.append(df)\n",
    "                \n",
    "        combined_df_train = pd.concat(df_train_list)\n",
    "        combined_df_test = pd.concat(df_test_list)\n",
    "        datasets[dataset_folder] = {\n",
    "            \"train\": combined_df_train,\n",
    "            \"test\": combined_df_test,\n",
    "        }\n",
    "    return datasets \n",
    "\n",
    "def trim_cgm_data(df):\n",
    "    # Group by 'id' to apply trimming for each unique id\n",
    "    trimmed_dfs = []\n",
    "    for unique_id in df['id'].unique():\n",
    "        # Select rows for the current id\n",
    "        id_data = df[df['id'] == unique_id]\n",
    "\n",
    "        # Find the first and last non-NaN index in the 'cgm' column\n",
    "        first_valid_index = id_data['CGM'].first_valid_index()\n",
    "        last_valid_index = id_data['CGM'].last_valid_index()\n",
    "\n",
    "        # If valid indices are found, trim the DataFrame\n",
    "        if first_valid_index is not None and last_valid_index is not None:\n",
    "            trimmed_id_data = id_data.loc[first_valid_index:last_valid_index]\n",
    "            trimmed_dfs.append(trimmed_id_data)\n",
    "\n",
    "    # Concatenate all trimmed DataFrames\n",
    "    return pd.concat(trimmed_dfs, ignore_index=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T16:28:20.820716Z",
     "start_time": "2024-11-03T16:28:20.800870Z"
    }
   },
   "id": "b6f985cd8d9f57f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c1cc61de03411ef"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1DEXI\n",
      "T1DEXIP\n",
      "tidepool_dataset\n",
      "OhioT1DM\n"
     ]
    }
   ],
   "source": [
    "datasets = get_datasets()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T15:31:31.743849Z",
     "start_time": "2024-11-03T15:31:03.306585Z"
    }
   },
   "id": "7697fb43824b8cc6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Validation\n",
    "Checking whether features are within a realistic range."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16835fbe899ef900"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY:  T1DEXI\n",
      "CGM: 0 samples outside the range 10-540\n",
      "bolus: 0 samples outside the range 0-100\n",
      "basal: 6 samples outside the range 0-10\n",
      "heartrate: 0 samples outside the range 20-300\n",
      "KEY:  T1DEXIP\n",
      "CGM: 1 samples outside the range 10-540\n",
      "bolus: 0 samples outside the range 0-100\n",
      "basal: 12 samples outside the range 0-10\n",
      "heartrate: 0 samples outside the range 20-300\n",
      "KEY:  tidepool_dataset\n",
      "CGM: 0 samples outside the range 10-540\n",
      "carbs: 2 samples outside the range 0-300\n",
      "bolus: 0 samples outside the range 0-100\n",
      "basal: 22570 samples outside the range 0-10\n",
      "KEY:  OhioT1DM\n",
      "CGM: 0 samples outside the range 10-540\n",
      "carbs: 1 samples outside the range 0-300\n",
      "bolus: 0 samples outside the range 0-100\n",
      "basal: 0 samples outside the range 0-10\n",
      "heartrate: 0 samples outside the range 20-300\n"
     ]
    }
   ],
   "source": [
    "ranges = {\n",
    "    'CGM': (10, 540),\n",
    "    'carbs': (0, 300),\n",
    "    'bolus': (0, 100),\n",
    "    'basal': (0, 10),\n",
    "    'heartrate': (20, 300)\n",
    "}\n",
    "\n",
    "for key in datasets:\n",
    "    print(\"KEY: \", key)\n",
    "    train_df = datasets[key]['train']\n",
    "    test_df = datasets[key]['test']\n",
    "    combined_df = pd.concat([train_df, test_df])\n",
    "\n",
    "    for feature, (min_val, max_val) in ranges.items():\n",
    "        if feature in combined_df.columns:\n",
    "            # Count values outside the specified range\n",
    "            outside_range_count = combined_df[(combined_df[feature] < min_val) | (combined_df[feature] > max_val)].shape[0]\n",
    "            print(f\"{feature}: {outside_range_count} samples outside the range {min_val}-{max_val}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T15:50:52.098674Z",
     "start_time": "2024-11-03T15:50:49.727492Z"
    }
   },
   "id": "9d83bba3b07c7ffa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6712d680990594d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY:  T1DEXI\n",
      "Percentage of non-NaN values after trimming: 35.13%\n",
      "KEY:  T1DEXIP\n",
      "Percentage of non-NaN values after trimming: 17.55%\n",
      "KEY:  tidepool_dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Trim the 'test' DataFrame\u001B[39;00m\n\u001B[1;32m     10\u001B[0m test_df \u001B[38;5;241m=\u001B[39m datasets[key][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 11\u001B[0m trimmed_test_df \u001B[38;5;241m=\u001B[39m \u001B[43mtrim_cgm_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m combined_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([train_df, test_df])\n\u001B[1;32m     15\u001B[0m total_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(combined_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCGM\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Cell \u001B[0;32mIn[18], line 54\u001B[0m, in \u001B[0;36mtrim_cgm_data\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     51\u001B[0m         trimmed_dfs\u001B[38;5;241m.\u001B[39mappend(trimmed_id_data)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# Concatenate all trimmed DataFrames\u001B[39;00m\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrimmed_dfs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Repositories/BGP_DatasetMerge/venv/lib/python3.11/site-packages/pandas/util/_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[1;32m    310\u001B[0m     )\n\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Repositories/BGP_DatasetMerge/venv/lib/python3.11/site-packages/pandas/core/reshape/concat.py:294\u001B[0m, in \u001B[0;36mconcat\u001B[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;129m@deprecate_nonkeyword_arguments\u001B[39m(version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, allowed_args\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjs\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconcat\u001B[39m(\n\u001B[1;32m     92\u001B[0m     objs: Iterable[NDFrame] \u001B[38;5;241m|\u001B[39m Mapping[Hashable, NDFrame],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    101\u001B[0m     copy: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    102\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m FrameOrSeriesUnion:\n\u001B[1;32m    103\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m    along the other axes.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001B[39;00m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 294\u001B[0m     op \u001B[38;5;241m=\u001B[39m \u001B[43m_Concatenator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjoin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    299\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    300\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    301\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    302\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverify_integrity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify_integrity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    304\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result()\n",
      "File \u001B[0;32m~/Documents/Repositories/BGP_DatasetMerge/venv/lib/python3.11/site-packages/pandas/core/reshape/concat.py:351\u001B[0m, in \u001B[0;36m_Concatenator.__init__\u001B[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[0m\n\u001B[1;32m    348\u001B[0m     objs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(objs)\n\u001B[1;32m    350\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(objs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 351\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo objects to concatenate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    353\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m keys \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    354\u001B[0m     objs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(com\u001B[38;5;241m.\u001B[39mnot_none(\u001B[38;5;241m*\u001B[39mobjs))\n",
      "\u001B[0;31mValueError\u001B[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Get a new dataset where there are no nan CGMs in beginning or end\n",
    "for key in datasets:\n",
    "    print(\"KEY: \", key)\n",
    "    \n",
    "    # Trim the 'train' DataFrame\n",
    "    train_df = datasets[key]['train']\n",
    "    trimmed_train_df = trim_cgm_data(train_df)\n",
    "\n",
    "    # Trim the 'test' DataFrame\n",
    "    test_df = datasets[key]['test']\n",
    "    trimmed_test_df = trim_cgm_data(test_df)\n",
    "    \n",
    "    combined_df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    total_count = len(combined_df['CGM'])\n",
    "    non_nan_count = combined_df['CGM'].notna().sum()\n",
    "    percentage = (non_nan_count / total_count) * 100\n",
    "    print(f\"Percentage of non-NaN values after trimming: {percentage:.2f}%\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T16:36:12.775794Z",
     "start_time": "2024-11-03T16:33:04.505097Z"
    }
   },
   "id": "882c6990694dcffb"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T16:24:51.351332Z",
     "start_time": "2024-11-03T16:24:51.328571Z"
    }
   },
   "id": "ebeb3f37e8f00635"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cde6634c809ed58e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5fdf84e2f97391c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "429900b8be584ae4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "84e7a58c8aa4c402"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "410aa5ee2e2a49ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ac4436c51d49c21a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f6fc96c203dbda"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
